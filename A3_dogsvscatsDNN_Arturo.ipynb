{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><center><b>TECNOLÓGICO DE MONTERREY</b></center></font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"5\"><center><b>Sistemas neuronales</b></center></font>\n",
    "\n",
    "<font size=\"5\"><center><b>(Deep Learning)</b></center></font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"5\"><center><em> Profr. Dr. Leonardo Chang</em></center></font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"5\"><center><b>Hands on activity</b></center></font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"4\"><center><em>Authors:</em></center></font>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<font size=\"5\"><center><em> Jesús Leopoldo Llano García (A01748867)</em></center></font>\n",
    "\n",
    "<font size=\"5\"><center><em> Ariel Arturo Gamboa Vázquez  (A01749802)</em></center></font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"4\"><center> March 5, 2020</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "__Instructions:__\n",
    "- You will use this Keras program to perform binary classification experiments on the Dogs and Cats dataset.\n",
    "- You can download the original dataset from www.kaggle.com/c/dogs-vs-cats/data (you’ll need to create a Kaggle account if you don’t already have one—don’t worry, the process is painless).\n",
    "- Adjust the following hyperparameters:\n",
    "    - Number of hidden layers\n",
    "    - Number of hidden units\n",
    "    - Learning rate\n",
    "    - Mini-batch size\n",
    "- Use two or more regularization methods.\n",
    "- Discuss the effect of these hyperparameters and regularization in the performance of classification and in the model overfitting.\n",
    "\n",
    "__Evaluation ponderation:__\n",
    "- Hyperparameters tuning and corresponding discussion ---- 50%\n",
    "- Regularization and corresponding discussion ------------ 50%\n",
    "- Total ---------------------------------------------------100%\n",
    "\n",
    "Code reference: https://github.com/fchollet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2151,
     "status": "ok",
     "timestamp": 1583204365699,
     "user": {
      "displayName": "Jesús Leopoldo Llano García",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggh50_e9Hk0sAIwY10hcKorQi0ew_DDbDNtDplE=s64",
      "userId": "11166401614180834702"
     },
     "user_tz": 360
    },
    "id": "mJtLJoRWfawO",
    "outputId": "c82f23ce-af08-47e3-886f-5464d6f67fa2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/guby/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models, layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xiM2S1mfawT"
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runthis if you already have run the whole thing and you dont wanna f it up\n",
    "\n",
    "original_dataset_dir = '/home/guby/Documents/Playground/A3-20200303T030127Z-001/Handson6/Data/train'\n",
    "\n",
    "base_dir = 'CatsNDogs'\n",
    "\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4551,
     "status": "error",
     "timestamp": 1583115068171,
     "user": {
      "displayName": "Jesús Leopoldo Llano García",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggh50_e9Hk0sAIwY10hcKorQi0ew_DDbDNtDplE=s64",
      "userId": "11166401614180834702"
     },
     "user_tz": 360
    },
    "id": "x4fCLOpZfawX",
    "outputId": "3dd39763-1290-42a7-fa05-ea50f3936631"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'CatsNDogs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f96691b6c78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# store our smaller dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CatsNDogs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Directories for our training,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'CatsNDogs'"
     ]
    }
   ],
   "source": [
    "# Please correct the urls accordingly\n",
    "\n",
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = '/home/guby/Documents/Playground/A3-20200303T030127Z-001/Handson6/Data/train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'CatsNDogs'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qo4w82X8fawc"
   },
   "outputs": [],
   "source": [
    "#VGG 16 network\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Conv2D(input_shape=(150,150,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=1, activation=\"softmax\"))\n",
    "\n",
    "from keras import optimizers\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 65,058,625\n",
      "Trainable params: 65,058,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guby/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/guby/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/30\n",
      " 35/100 [=========>....................] - ETA: 6:44 - loss: 7.9076 - acc: 0.4843"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A3_dogsvscatsDNN.ipynb",
   "provenance": [
    {
     "file_id": "1lCaBGCpjm4mw-bZ90hc4C3X-IqNLzmhn",
     "timestamp": 1583089583386
    }
   ]
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
